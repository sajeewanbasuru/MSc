The field of statistical learning theory has not only seen considerable advances in the last fifteen years, it has also found many applications, some of these appearing in commercial packages. It is now classified as a subfield of artificial intelligence, and as such gives an alternative, and frequently more general viewpoint on such topics as pattern recognition, regression estimation, and signal processing. The author of this book is one of the originators of statistical learning theory, and has written a book that will give the mathematically sophisticated reader a rigorous account of the subject. Most of the main results are proven in detail, but the author does find time to include insightful discussion on the origins and intuition behind the concepts involved in statistical learning theory.

Along with a brief introduction, the book consists of three parts, the first being an overview of the statistical theory of learning, the second giving the details of the now widely used support vector machines, and the last one (the most sophisticated mathematically) giving the statistical foundations of learning theory. In writing the book, the author wants to put forward a new approach to dependency estimation problems having their origin in learning theory, and being able to deal with the ?curse of dimensionality?. The origins of the subject lie in the pattern recognition problem and the Glivenko-Cantelli problem in statistics. Both of these problems were discovered to be essentially the same, and the author?s task is to use their similarities to construct a general theory of statistical inference and (inductive) learning. Indeed, a new induction principle, called ?structural risk minimization? (SRM) is paradigmatic in the book, along with the now ubiquitous VC dimension, the latter of which originates in the author?s early research. Both the SRM and the VC dimension illustrate the tension between the need for high accuracy and the need for the minimization of error in data sets.

The learning problem, as the author sees it, is the problem of selecting the correct dependence on the basis of empirical data. Two approaches to this problem are discussed, the first using a ?risk functional?, and the second involving the estimation of stochastic dependencies and the consequent solution of integral solutions. Both of these approaches are modeled in terms of a general model of learning from examples, which consists of a data generator, a supervisor, and a learning machine. The learning machine can either imitate the supervisor or identify how the supervisor operates. These two methods are different, the author says, in that the first one searches for the best prediction based on the data, while the second one attempts to approximate the operator representing the supervisor. Both approaches are studied in the book, with the first one being the easier of the two, while the second involving the solution of ill-posed problems. The author views the learning process in terms of choosing the right function from a given function collection.

Both perceptrons and their generalizations, neural networks, are briefly discussed in the book, along with the back-propagation method. The author gives reasons why he does not think neural networks are well-controlled learning machines, such as the existence of local minima, the slow convergence of the gradient method, and the choice of scaling factors. These problems serve as motivation for the introduction of support vector machines, which are introduced as optimal separating hyperplanes. Support vector machines take input vectors into a high-dimensional feature space via a nonlinear mapping, and an optimal separating hyperplane is then constructed in this feature space.

Similar to the need for neural networks to generalize well, separating hyperplanes must do the same, and due to the large dimensionality of the feature space, a hyperplane that separates the training data may not generalize well. In addition, the large dimensionality of the feature space makes the construction of the hyperplane computationally demanding. The author shows that optimal hyperplanes, found using various mathematical techniques such as quadratic optimization, do generalize well. Also, as the author points out, the explicit form of the feature space need not be known, since only the inner products between the ?support vectors? and the vectors of the feature space need to be calculated. The calculation of the inner product is done with the insight gained from Mercer?s theorem, which gives the existence of a kernel function such that there exists a feature space where this function generates the inner product. This inner product in feature space allows the construction of a decision function that is nonlinear in the input space but that is equivalent to a linear function in the feature space. Different choices of the kernel function give different types of learning machines. The author discusses three examples of support vector machines for pattern recognition: polynomial, radial basis function, and two-layer neural network support vector machines. An entire chapter is spent on the problem of digit recognition using support vector machines.